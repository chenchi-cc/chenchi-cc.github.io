---
layout: post
title: 简历项目
categories: 技术文档
description: 我的简历项目详细版本
keywords: 大数据，后台开发，算法
---
## 综述
简历项目共包括5部分：
- 数据治理&性能调优
- 离线数仓&ETL
- 实时计算
- 数据分析&AI
- 数据平台

## 数据治理&性能调优
- **技术栈：** Scala + Shell + Hadoop + Hive + Spark SQL + Scheduler
- **负责：**
  - PUSH方向的数据治理：主要包括代码治理、任务治理，规范治理，资源治理和时效性治理。
- **具体工作：**
  - **代码治理：** 原来PUSH离线任务多为MR任务，sql脚本杂乱无章，无版本概念，且多在无人维护的旧调度平台。将代码统一用spark3重写，提交到代码库进行管理，并迁移到新的调度平台。
    - 制定项目结构，做到代码分层和直接运行sql文件，参考：https://github.com/chenchi-cc/data-warehouse-etl-framework
    - 因成本问题去udw表（一种自研hive表），需要将非核心表迁移到HDFS读写。为提高易用性，将HDFS文件读写（parquet）统一抽象成公共方法，封装schema信息，支持多天查询，兼容通过新增的字段查询旧文件。
  - **任务治理：** 针对PUSH任务数据分层不合理，自助化率低的现状，进行了重构的工作。
    - 报表逻辑转移至数仓，去除无用中间层逻辑，重复字段的清洗复用同一套代码。
    - 公共代码抽取，降低代码耦合度，复杂逻辑全部通过udf重写。
    - 充分利用spark3特性，进行参数性能调优，调整并行度，去除逻辑中不等join，全部通过等值join来替代。
    - 进行分而治之的思想，减少shuffle，将能广播的任务，配置广播。
    - 引入remote shuffle service，将shuffle数据写到HDFS，防止单台机器被打满，在大规模数据量和非标准队列条件下，保障任务的稳定性。
    - 针对小文件场景，前置方案设计自适应的分区数（考虑压缩，数据量 / HDFS 块大小），调参动态合并小文件，采用高效parquet格式；后置方案编写脚本，自动化定期合并。
  - **规范治理：** 对设计、流程以及质量层面做了相应的规范性约束。
    - 设计规范：涵盖表命名，字段命名，数仓模型建设，代码层级规范等。
    - 流程规范：针对过往排期混乱、需求滞后偏多等问题，规范化了提需流程，需求插入流程，开发流程以及上线流程等。
    - 质量规范：核心任务全部配置DQC监控，前置在数仓产出前会进行量级校验，后置在报表指标层会对核心指标波动报警；任务失败和延迟都配置了对应的值班机制。
  - **资源治理：** 物理机，计算队列，HDFS存储均会有不同程度的打满，配置了多层监控以及自动清理脚本，推动值班机制进行可靠性保障。
    - 针对计算队列多而杂的问题，将同地域合并，将队列划分为线上队列（核心+非核心），测试队列和回溯队列；
    - 针对HDFS存储资源不够现状，进行地域置换，跨地域迁移（distcp），尽量保证和计算队列同一地域；
    - 针对全量任务梳理，下线无用的任务，清理存储。
    - 时效性治理：缩短数仓链路，解决不合理的数据依赖，推动上游签署SLA，切换更快产出的数据源，通过性能调优缩短任务本身的执行时间。
- **难点：**
  - **数据倾斜问题优化：** 针对Group By场景以及Join场景（大表join大表，大表join小表）进行优化，提升任务的稳定性和时效性。
    - 第一步：脏数据的过滤，union数据类型的统一，压缩的调整（当对文件使用GZIP压缩等不支持文件分割操作的压缩方式，在日后有作业涉及读取压缩后的文件时，该压缩文件只会被一个任务所读取。如果该压缩文件很大，则处理该文件的Map需要花费的时间会远多于读取普通文件的Map时间）
    - 第二步：想尽一切办法避免shuffle，消除Exchange
      - 看小表能否被广播出去：spark.sql.autoBroadcastJoinThreshold默认10MB，调整成2G或更大；或者利用Join Hint强制广播。需要预估小表的大小（磁盘+内存）。限制是不能full outer join，也不能广播外表（例如左关联，广播左边的表）。
      - 小表不能广播：看能否过滤后被广播。常见的嵌套查询、子查询利用filter做谓词下推；多个JOIN的场景，就要用AQE（spark.sql.adaptive.enabled=true）的JOIN策略调整（DemoteBroadcastHashJoin），基于shuffle map的中间值。
      - 小表不能广播：Join Key 远大于 Payload。多个Join Key进行合并，两次hash取值连接，进行join。这样小表就能被广播出去了。
      - 小表不能广播：小表拆分列，一般按日期，多个和大表关联，每一个都转成广播。大表要避免重复扫描：cache住或者利用DPP机制。
      - 分桶join（Bucket join），本身也是一种shuffle free的方式，同时还能通过存储桶修剪（Bucket pruning）功能减少IO。需要注意的是，必须按同一个join key做分桶表，且分桶数量要大于等于shuffle分区数量，否则至少一侧还是会触发shuffle（Spark 3.1.1做了增强，不考虑分区数量，只要两者成倍数就能shuffle free）。
    - 第三步：必须shuffle的情况，分为Task粒度和Executor粒度。Task粒度开启AQE即可，重点说下Executor粒度，思想就是：加盐+两阶段shuffle。
      - Group By场景：非去重类指标直接通过随机数加盐，去重类指标需要按照去重id哈希取模来加盐。同时如果只有几个大key可以单独进行处理。
      - Join场景：如果是一个表有很多大key，另一个表比较均匀，将数据倾斜表全部加盐，另外一个表数据增到到原来的N倍（随机数个数，笛卡尔积），然后将二者Join并去盐。同时如果只有几个大key可以单独进行处理。
  - **维度爆炸背景下uv计算实践：** 优化大数据计算中多维度用户数统计的方法，通过数据打标的方式避免数据膨胀和冗余传输，同时通过编号化结果维度信息，采用更小的数据结构进行存储，提高性能并减少计算成本。
    - 具体参考：https://mp.weixin.qq.com/s/kHfc2783V5GV3dxWU95odQ
    - PUSH增量数据发送为160亿，到达为60亿，点击为3000万，其中发送数据一天增量约为16T，在做报表聚合时，需要从不同维度去看对应的用户数，由于用户数不同维度不可累加的特性，基本上所有维度的用户数都需要单独计算，维度少可以直接count distinct计算，但是维度多的话这种计算相当痛苦。
    - 常规方式按照 cuid+初始维度 去重后，使用 lateral view explode(array(维度名, 'all')) 将数据从一行膨胀成多行，然后直接 distinct 的数据计算方式。新的优化思路可以理解为在数据处理过程中采用一种“数据打标”策略，通过在数据去重的基础上生成用户粒度的中间数据，并在此基础上动态附加所需的结果维度信息。
    - 通过将不同维度（含all）的组合进行维度编码，然后将维度编码聚合到用户cuid粒度上，最后再拆解聚合后的维度编码去计算uv，理论上整个计算过程数据量是呈收敛聚合的，不会膨胀。例如001用户有两条数据，分别是产品线=1 & 付费类型=免费，和产品线=1 & 付费类型=付费，那么第一条数据就会编码为((1,all),(all,免费),(1,免费),(all,all))，我们不妨简化为(1,3,5,9)，同理第二条数据为(1,4,6,9)，这样聚合到用户粒度就是001用户的编码集为(1,3,4,5,6,9)，反过来1，3，4，5，6，9每个维度组合都有用户001。
    - 具体计算可以先利用dense_rank生成编码集，维度集含有dim_A,...,dim_N的具体值以及对应的编号，将该数据集广播，去和原日志表join（key就是多个dim）获取cuid，如果dim过多超过阈值无法广播，可以把所有join key拼接在一起去进行哈希（防止碰撞可考虑二次哈希），然后利用hash key进行关联，cuid聚合编码集可利用array_distinct函数。
    - 执行时间可由原2小时缩短到30分钟内，且任务稳定性增加，随着维度的增加，性能优势逐渐明显。
- **效果：**
  - 核心数据产出时效由T+1日22点提升至T+1日12点；
  - 减少研发临时跑数1人力的工作量，需求完成率达到100%；
  - 减少70%以上的冗余代码，计算资源减少50%以上；
  - 任务的稳定性得到极大提升，核心任务SLA满足率100%。