---
layout: post
title: 简历项目
categories: 技术文档
description: 我的简历项目详细版本
keywords: 大数据，后台开发，算法
---
## 综述
简历项目共包括5部分：
- 数据治理&性能调优
- 离线数仓&ETL
- 实时计算
- 数据分析&AI
- 数据平台

## 数据治理&性能调优
- **技术栈：** Scala + Shell + Hadoop + Hive + Spark SQL + Scheduler
- **负责：**
  - PUSH方向的数据治理：主要包括代码治理、任务治理，规范治理，资源治理和时效性治理。
- **具体工作：**
  - **代码治理：** 原来PUSH离线任务多为MR任务，sql脚本杂乱无章，无版本概念，且多在无人维护的旧调度平台。将代码统一用spark3重写，提交到代码库进行管理，并迁移到新的调度平台。
    - 制定项目结构，做到代码分层和直接运行sql文件，参考：[https://github.com/chenchi-cc/data-warehouse-etl-framework](https://github.com/chenchi-cc/data-warehouse-etl-framework)
    - 因成本问题去udw表（一种自研hive表），需要将非核心表迁移到HDFS读写。为提高易用性，将HDFS文件读写（parquet）统一抽象成公共方法，封装schema信息，支持多天查询，兼容通过新增的字段查询旧文件。
  - **任务治理：** 针对PUSH任务数据分层不合理，自助化率低的现状，进行了重构的工作。
    - 报表逻辑转移至数仓，去除无用中间层逻辑，重复字段的清洗复用同一套代码。
    - 公共代码抽取，降低代码耦合度，复杂逻辑全部通过udf重写。
    - 充分利用spark3特性，进行参数性能调优，调整并行度，去除逻辑中不等join，全部通过等值join来替代。
    - 进行分而治之的思想，减少shuffle，将能广播的任务，配置广播。
    - 引入remote shuffle service，将shuffle数据写到HDFS，防止单台机器被打满，在大规模数据量和非标准队列条件下，保障任务的稳定性。
    - 针对小文件场景，前置方案设计自适应的分区数（考虑压缩，数据量 / HDFS 块大小），调参动态合并小文件，采用高效parquet格式；后置方案编写脚本，自动化定期合并。
  - **规范治理：** 对设计、流程以及质量层面做了相应的规范性约束。
    - 设计规范：涵盖表命名，字段命名，数仓模型建设，代码层级规范等。
    - 流程规范：针对过往排期混乱、需求滞后偏多等问题，规范化了提需流程，需求插入流程，开发流程以及上线流程等。
    - 质量规范：核心任务全部配置DQC监控，前置在数仓产出前会进行量级校验，后置在报表指标层会对核心指标波动报警；任务失败和延迟都配置了对应的值班机制。
  - **资源治理：** 物理机，计算队列，HDFS存储均会有不同程度的打满，配置了多层监控以及自动清理脚本，推动值班机制进行可靠性保障。
    - 针对计算队列多而杂的问题，将同地域合并，将队列划分为线上队列（核心+非核心），测试队列和回溯队列；
    - 针对HDFS存储资源不够现状，进行地域置换，跨地域迁移（distcp），尽量保证和计算队列同一地域；
    - 针对全量任务梳理，下线无用的任务，清理存储。
    - 时效性治理：缩短数仓链路，解决不合理的数据依赖，推动上游签署SLA，切换更快产出的数据源，通过性能调优缩短任务本身的执行时间。
- **难点：**
  - **数据倾斜问题优化：** 针对Group By场景以及Join场景（大表join大表，大表join小表）进行优化，提升任务的稳定性和时效性。
    - 第一步：脏数据的过滤，union数据类型的统一，压缩的调整（当对文件使用GZIP压缩等不支持文件分割操作的压缩方式，在日后有作业涉及读取压缩后的文件时，该压缩文件只会被一个任务所读取。如果该压缩文件很大，则处理该文件的Map需要花费的时间会远多于读取普通文件的Map时间）
    - 第二步：想尽一切办法避免shuffle，消除Exchange
      - 看小表能否被广播出去：spark.sql.autoBroadcastJoinThreshold默认10MB，调整成2G或更大；或者利用Join Hint强制广播。需要预估小表的大小（磁盘+内存）。限制是不能full outer join，也不能广播外表（例如左关联，广播左边的表）。
      - 小表不能广播：看能否过滤后被广播。常见的嵌套查询、子查询利用filter做谓词下推；多个JOIN的场景，就要用AQE（spark.sql.adaptive.enabled=true）的JOIN策略调整（DemoteBroadcastHashJoin），基于shuffle map的中间值。
      - 小表不能广播：Join Key 远大于 Payload。多个Join Key进行合并，两次hash取值连接，进行join。这样小表就能被广播出去了。
      - 小表不能广播：小表拆分列，一般按日期，多个和大表关联，每一个都转成广播。大表要避免重复扫描：cache住或者利用DPP机制。
      - 分桶join（Bucket join），本身也是一种shuffle free的方式，同时还能通过存储桶修剪（Bucket pruning）功能减少IO。需要注意的是，必须按同一个join key做分桶表，且分桶数量要大于等于shuffle分区数量，否则至少一侧还是会触发shuffle（Spark 3.1.1做了增强，不考虑分区数量，只要两者成倍数就能shuffle free）。
    - 第三步：必须shuffle的情况，分为Task粒度和Executor粒度。Task粒度开启AQE即可，重点说下Executor粒度，思想就是：加盐+两阶段shuffle。
      - Group By场景：非去重类指标直接通过随机数加盐，去重类指标需要按照去重id哈希取模来加盐。同时如果只有几个大key可以单独进行处理。
      - Join场景：如果是一个表有很多大key，另一个表比较均匀，将数据倾斜表全部加盐，另外一个表数据增到到原来的N倍（随机数个数，笛卡尔积），然后将二者Join并去盐。同时如果只有几个大key可以单独进行处理。
  - **维度爆炸背景下uv计算实践：** 优化大数据计算中多维度用户数统计的方法，通过数据打标的方式避免数据膨胀和冗余传输，同时通过编号化结果维度信息，采用更小的数据结构进行存储，提高性能并减少计算成本。
    - 具体参考：[https://mp.weixin.qq.com/s/kHfc2783V5GV3dxWU95odQ](https://mp.weixin.qq.com/s/kHfc2783V5GV3dxWU95odQ)
    - PUSH增量数据发送为160亿，到达为60亿，点击为3000万，其中发送数据一天增量约为16T，在做报表聚合时，需要从不同维度去看对应的用户数，由于用户数不同维度不可累加的特性，基本上所有维度的用户数都需要单独计算，维度少可以直接count distinct计算，但是维度多的话这种计算相当痛苦。
    - 常规方式按照 cuid+初始维度 去重后，使用 lateral view explode(array(维度名, 'all')) 将数据从一行膨胀成多行，然后直接 distinct 的数据计算方式。新的优化思路可以理解为在数据处理过程中采用一种“数据打标”策略，通过在数据去重的基础上生成用户粒度的中间数据，并在此基础上动态附加所需的结果维度信息。
    - 通过将不同维度（含all）的组合进行维度编码，然后将维度编码聚合到用户cuid粒度上，最后再拆解聚合后的维度编码去计算uv，理论上整个计算过程数据量是呈收敛聚合的，不会膨胀。例如001用户有两条数据，分别是产品线=1 & 付费类型=免费，和产品线=1 & 付费类型=付费，那么第一条数据就会编码为((1,all),(all,免费),(1,免费),(all,all))，我们不妨简化为(1,3,5,9)，同理第二条数据为(1,4,6,9)，这样聚合到用户粒度就是001用户的编码集为(1,3,4,5,6,9)，反过来1，3，4，5，6，9每个维度组合都有用户001。
    - 具体计算可以先利用dense_rank生成编码集，维度集含有dim_A,...,dim_N的具体值以及对应的编号，将该数据集广播，去和原日志表join（key就是多个dim）获取cuid，如果dim过多超过阈值无法广播，可以把所有join key拼接在一起去进行哈希（防止碰撞可考虑二次哈希），然后利用hash key进行关联，cuid聚合编码集可利用array_distinct函数。
    - 执行时间可由原2小时缩短到30分钟内，且任务稳定性增加，随着维度的增加，性能优势逐渐明显。
- **效果：**
  - 核心数据产出时效由T+1日22点提升至T+1日12点；
  - 减少研发临时跑数1人力的工作量，需求完成率达到100%；
  - 减少70%以上的冗余代码，计算资源减少50%以上；
  - 任务的稳定性得到极大提升，核心任务SLA满足率100%。

## 离线数仓&ETL
- **技术栈：** Scala + Java + Shell + Hadoop + Hive + Spark SQL + Doris + ClickHouse + MySQL + SpringBoot（SSM）
- **负责：**
  - PUSH数仓基建，主要采用的Kimball维度建模，实时和离线采用的Lambda架构，后续离线和实时共用一套ODS层数据。
  - 百家号大B端数仓基建，主要采用的宽表建模（进一步降低冗余，减少存储，增强易用性），核心字段来自于实时数据（全量同步+增量同步），采用列式存储和高效压缩，降低了存储，事实表与维度表进一步打平。
    - 数仓具体参考：[https://mp.weixin.qq.com/s/z3gXT53Hl2t5REdhlZGo9w](https://mp.weixin.qq.com/s/z3gXT53Hl2t5REdhlZGo9w)
    - 数据产品为CMS内容分析平台：[https://baijiahao.baidu.com/](https://baijiahao.baidu.com/)，负责ToC数据的展示
- **具体工作：**
  - PUSH数仓按照明确数据域，构建业务总线矩阵，明确统计指标的步骤整体设计的，下游使用方主要是是报表层和算法策略数据。
    - 横向分层为ODS、DWD、DWS、ADS以及DIM层；纵向划分数据域，包括流量域、用户域和物料域；
    - 流量域是行为日志，含有发送、到达、点击、首调归因4种行为，为事务型事实表；用户域来源于日活用户，物料域来源于业务底表，均作为维度表使用（可理解为全量快照表），维度设计便于统计，采用星型模型；
    - 明确行为日志粒度为cuid（设备id）+appid（设备APP）+taskid（物料id），根据4种业务过程和维度信息构建业务总线矩阵；
    - 明确统计指标，划分原子、派生和衍生指标，其中派生指标为原子+统计周期+业务限定+统计粒度，衍生指标为复合指标逻辑（比率），进行指标的复用；
    - DWS汇总层继承了明细层的粒度，JOIN上了全量维度信息，在其上ADS层服务于报表会进一步上卷（多为Doris和CK表）；
  - 百家号数仓数据按照宽表建模，简化了分层和数据域的概念，减少数据分层，底层多为文件，上层只分为作者和文章两大主题宽表（明细粒度），将多个业务域（百家号、动态、好看、feed）统一，解决了数据孤岛问题，宽表分多版本产出，会将宽表同步CK，方便使用方一站式自助分析。
    - 数据孤岛问题：原始数仓各个业务口径、业务覆盖差异，数据流通、业务沟通、数据开发成本很高，通过责权分明和技术统一，做了高效的融合，融合后的宽表含有1000+字段，口径更清晰，同时业务使用上更方便，效率更高；
    - 木桶效应问题：30+上游，到位时间不同会形成木桶效应，为了复杂逻辑进行解耦，引入了分版本输出方案（见难点1）；
    - 存储冗余问题：简化多层数仓，降低冗余，总存储下降30%左右；
    - 回溯成本问题：开发通用型工具，迭代需要更新的字段，其他字段取原表数据，merge在一起后写入临时目录，最后再rename回原表目录；
- **难点：**
  - **百家号数仓分版本产出方案：** 宽表通过设置多个版本，分阶段产出不同类型的数据，下游使用方通过对应的版本产出标识配置依赖（可以根据sql字段自动提取对应的版本）。
    - V1表是数仓的base表，作为整个数仓的核心数据，包含核心主键以及相关字段，最先产出，非V1版本的其他字段置NULL；
    - 其他版本V2-Vn，彼此间没有先后关系，谁先依赖就绪谁先运行，先把当前版本数据写到HDFS目录，然后通过主键merge回原表（非当前版本的字段取表中的原值）；
    - 每个版本会有一个当前版本字段的配置文件，获取表元数据metadata的全量字段，两者diff就是非当前版本的字段，配置文件会接入数据平台服务，方便配置依赖关系；
    - 多个版本同时merge会出现数据异常，一方面限制版本的数量，另一方面会通过 表+分区 粒度进行事务隔离，具体如下：
      - 表+分区粒度在MySQL中存放版本锁标识，每次任务merge前先查看版本锁是否占用，没占用就加版本锁，然后执行merge，执行完、占用时间超时或者异常都会释放；
      - 如果当前版本锁已被占用，则进行等待，按照3分钟的频次轮询；
      - 极端情况，两个事务线程同时查看版本锁状态，MySQL InnoDB默认采用REPEATABLE READ隔离级别，通过MVCC乐观锁使得读写之间不阻塞，在第一次SELECT时生成并复用该ReadView，但是可能造成两个事务同时获取未加版本锁的状态。因此将采用MySQL中的排他锁，在SELECT的时候加上FOR UPDATE；
  - **百家号活动作者应用数据：** 每个百家号作者在活动期间仅会引入一次，引入日期固定，需要计算每位作者自引入日期开始，当月、次1月......一直到次12月的留存、发文和分发情况，“月”按照滚动30日窗口计算，不满一整月按实际天数计算。
    - 留存和发文可根据快照表时间判定是否活跃和是否发文，分发需要关联文章行为事实数据，为增量数据，关联多少天分区根据实际而定；
    - 考虑到未来扩展性，当月，次1、次2采用一个新列period_type来表示，分别取值为0-12；
    - 每位作者的时间周期都不相同，实际是按照每位作者做一个窗口大小为30（不满30按照实际天数）的滚动窗口；
    - 经分析每位作者只有在最近30天的数据有变化，其余周期数据固定不变，为避免重复计算，需做成与前一日增量聚合的效果；
    - 因每位作者时间周期不同，在最近30天的时间范围也不同，因此关联是不等join，优化为提前算最近1-30日的数据，采用等值join进行关联；
- **效果：**
  - **PUSH数仓：** 支持下游工程团队分析和算法团队模型训练，促进了PUSH dau的增长。
  - **百家号数仓：** 需求平均交付时长由4.7d下降为2.8d，数仓存储平均节省了2.6PB，易用性得到极大的提升。
  - 整体达到高效率，高性能，高质量，低成本的数仓建设目标。